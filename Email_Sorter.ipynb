{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14f1736d",
   "metadata": {},
   "source": [
    "# Email Importance Recognition\n",
    "\n",
    "Personally, I get a lot of emails that are very irrelevant to me. And no matter how many I unsubscribe from, more always come. And in the process of getting rid of all of this junk, I often find that something important gets lost in the mix. So I have made a process using Google's Gemini 2.0 flash model to predict how important an email is to a given user.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "Ideally, we would use an API call to automatically retrieve emails from a service. However, if we wish to use a service like Microsoft Outlook or Gmail, this requires verified OAuth2 access, which is a little out-of-scope for this project. And unfortunately, a list of example emails was out of the question as well, as they do not come with relevance to a given user. So in the end, I used a manually assembled list of 12 emails straight from my own inbox to test on.\n",
    "\n",
    "Additionally, I have included an example 'User Preferences' string that resembles something I might have used for myself.\n",
    "\n",
    "### Loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af1b3dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "emails = {}\n",
    "with open(\"sample_emails.json\", \"r\") as f:\n",
    "    emails = json.load(f)\n",
    "emails, user_pref = emails[\"Emails\"], emails[\"UserPreferences\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e72dff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "993ba2ec",
   "metadata": {},
   "source": [
    "## Assessing the Importance\n",
    "\n",
    "In order to categorize these emails, we first need a pre-trained model to work with (I will certainly not be able to train a model on my own to have complex and high-order reasoning skill and a solid grasp on language from only 12 emails)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a6cf9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import torch\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "key = open(\"C:\\\\Users\\\\peter\\\\OneDrive\\\\Documents\\\\GeminiAPI.txt\", \"r\").read() # Replace with your key\n",
    "client = genai.Client(api_key=key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fafb327",
   "metadata": {},
   "source": [
    "## The Measure\n",
    "\n",
    "This whole time I have been mentioning 'rating importance', but how can this actually be accomplished?\n",
    "There are 2 main ways to approach this:\n",
    "- Relative ordering\n",
    "- Giving a direct score\n",
    "\n",
    "Relative ordering involves giving the AI 2 emails at once and having the model chose which is more important. This is likely a better approach in terms of results, as the model will be better at this task than a somewhat arbitrary score, but it has it's downsides. Namely that it will often involve feeding the same email to the model multiple times, which requires it to parse more tokens, which is not something we have unlimited access of for our free model. Additionally, it is a more complex method.\\\n",
    "\\\n",
    "For these reasons, I went with a more direct approach: simply asking the model for a score given an email and the user preferences. While this method is cheap and easy, it can lack nuance. Trying to define the scale for the scores can be challenging, especially when trying to phrase them in a way the model will truly grasp and understand.\\\n",
    "\\\n",
    "In the end, I decided on a scale of 1 to 5 (with 5 being the most important), and instructed the model to only use '1' and '5' in extreme cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00c4a810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rating(email, UserPreference=\"\"):\n",
    "    input = f\"\"\"\n",
    "        Please give an integer rating of 1, 2, 3, 4, or 5 representing how important the following email would be to the user (5 is the most important). Do not output anything else, no explanation is needed.\n",
    "        Try to reserve 1 for truly useless emails, and 5 for very high priority emails. Emails that could potentially be useful or important to the user should on average receive a 3.\n",
    "        The user may also provide some details about them that would influence what is or is not important to them.\n",
    "        Details about the user: {UserPreference}\n",
    "        \n",
    "        Email:\n",
    "        From: {email['from']}\n",
    "        Subject: {email['subj']}\n",
    "        \n",
    "        {email['body']}\"\"\"\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=input,\n",
    "    )\n",
    "    \n",
    "    res = response.text[0]\n",
    "    try:\n",
    "        res = int(res)\n",
    "    except:\n",
    "        res = 0\n",
    "        print(\"Error parsing response. Expected integer, got: \", end=\"\")\n",
    "        print(res)\n",
    "        print(f\"({response.text})\")\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d43268",
   "metadata": {},
   "source": [
    "And now, to run the test for 1 example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac43e1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email:\n",
      "From: OneDrive <no-reply@onedrive.com>\n",
      "Subject: Many files were recently deleted from your OneDrive\n",
      "\n",
      "Files are permanently removed from your OneDrive recycle bin 30 days after they ...\n",
      "Model Rating: 3/5\n"
     ]
    }
   ],
   "source": [
    "print(f\"Email:\\nFrom: {emails[0]['from']}\\nSubject: {emails[0]['subj']}\\n\\n{emails[0]['body'][:80]}...\")\n",
    "print(f\"\\n\\nModel Rating: {compute_rating(emails[0], user_pref)}/5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7523cbd7",
   "metadata": {},
   "source": [
    "## Scoring the results\n",
    "\n",
    "Great! Now we have a way to give emails a level of importance based on a given user. But how can we even tell if it is doing a good job? For this, we need to figure out some sort of metric.\n",
    "\n",
    "### The metric\n",
    "\n",
    "For this metric, I decided to simply check how close the model was to what the correct result was. In order to make this correct result, we can simply go in and do the model's job for it once - giving every email an importance level we think appropriate. Once this is done, we can run the model again for every email, and see how close it was to the real answer. Then, we can take the Mean Absolute Error (or the Total Absolute Error) and use this to fine tune any aspect of the model or the input we provide to it.\\\n",
    "\\\n",
    "(Using threading for parallel operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7802635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "\n",
    "results = {}\n",
    "res_sem = threading.Semaphore()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_rating_parallel(email, i, UserPreference=\"\"):\n",
    "    input = f\"\"\"\n",
    "        Please give an integer rating of 1, 2, 3, 4, or 5 representing how important the following email would be to the user (5 is the most important). Do not output anything else, no explanation is needed.\n",
    "        Try to reserve 1 for truly useless emails, and 5 for very high priority emails. Emails that could potentially be useful or important to the user should on average receive a 3.\n",
    "        The user may also provide some details about them that would influence what is or is not important to them.\n",
    "        Details about the user: {UserPreference}\n",
    "        \n",
    "        Email:\n",
    "        From: {email['from']}\n",
    "        Subject: {email['subj']}\n",
    "        \n",
    "        {email['body']}\"\"\"\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=input,\n",
    "    )\n",
    "    \n",
    "    res = response.text[0]\n",
    "    try:\n",
    "        res = int(res)\n",
    "    except:\n",
    "        res = 0\n",
    "        print(\"Error parsing response. Expected integer, got: \", end=\"\")\n",
    "        print(res)\n",
    "        print(f\"({response.text})\")\n",
    "    \n",
    "    res_sem.acquire()\n",
    "    results[i] = res\n",
    "    res_sem.release()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_rating_for_all(emails, user_pref=\"\"):\n",
    "    global res_sem, results\n",
    "    \n",
    "    threads = []\n",
    "    \n",
    "    for i, email in enumerate(emails):\n",
    "        # dispatch a thread to compute the rating for the current email\n",
    "        time.sleep(0.1)  # sleep for a bit to avoid overwhelming the server\n",
    "        thread = threading.Thread(target=compute_rating_parallel, args=(email, i, user_pref))\n",
    "        thread.daemon = True\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "    \n",
    "    # yield results as they come\n",
    "    for i, thread in enumerate(threads):\n",
    "        thread.join()\n",
    "        res_sem.acquire()\n",
    "        yield results[i]\n",
    "        res_sem.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30c7699",
   "metadata": {},
   "source": [
    "And run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fbdb3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating 1/12: 3 (real: 3)\n",
      "Rating 2/12: 1 (real: 1)\n",
      "Rating 3/12: 1 (real: 2)\n",
      "Rating 4/12: 1 (real: 1)\n",
      "Rating 5/12: 1 (real: 1)\n",
      "Rating 6/12: 1 (real: 1)\n",
      "Rating 7/12: 2 (real: 1)\n",
      "Rating 8/12: 2 (real: 1)\n",
      "Rating 9/12: 4 (real: 4)\n",
      "Rating 10/12: 5 (real: 5)\n",
      "Rating 11/12: 1 (real: 3)\n",
      "Rating 12/12: 4 (real: 4)\n",
      "Average difference: 0.4166666666666667\n",
      "Total difference: 5\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "total_diff = 0\n",
    "for i, rating in enumerate(compute_rating_for_all(emails, user_pref)):\n",
    "    print(f\"Rating {i+1}/{len(emails)}: {results[i]} (real: {emails[i]['real_rating']})\")\n",
    "    total_diff += abs(results[i] - emails[i][\"real_rating\"])\n",
    "print(f\"Average difference: {total_diff/len(emails)}\")\n",
    "print(f\"Total difference: {total_diff}\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742298b6",
   "metadata": {},
   "source": [
    "## Refining the prompt\n",
    "\n",
    "I largely only used this difference result to fine-tune the prompt, but it can easily be used to measure the impact for any change to this process. I tried a lot of things, but found that my original prompt was almost spot-on. A few of the things I tried:\n",
    "- Reordering the prompt\n",
    "    - The order of the instructions, user preference, email metadata, and email body can be rearranged to yield different qualities of results. In the end though, I found that the obvious ordering of Explanation, User preference, Email metadata, then Email body was the best choice for accurate answers.\n",
    "- Rephrasing the instructions\n",
    "    - This was one of the main things I changed from the original prompt. Originally, the instructions were brief. The prompt simply stated that the model needed to output an integer from 1 to 5 to rank the importance of the following email, and that 5 was the most important. As it turns out, this gave the model a tendency to be overly fond of either a really high or really low rating, and to rarely give any other rating. I ended up needing to repeat this information and emphasize it in order to get the model to fully utilize the 1-5 scale.\n",
    "- Removing elements\n",
    "    - While most of the items included in the prompt seem necessary, the body of the email is not only one of the most token-heavy items, but is also potentially redundant, as it is summarized by the \"subject\" field by nature. However, my attempts to remove it to save on token usage had drastic negative impact on accuracy, and after some other tweaks to confirm this wasn't a fluke, the body was re-added.\n",
    "\n",
    "\n",
    "Overall, the changes I made managed to bring the model's Total Difference for this dataset down from 10 to 5, which is quite the improvement, considering a random number generator would theoretically average a Total Difference of 16.\n",
    "\n",
    "## Summary\n",
    "\n",
    "Overall, I would say that this concept has some serious potential. I know that personally, it could up my productivity with emails by a significant factor. The model works rather well, and I believe some more tweaks to the prompt, as well as switching to a more powerful model could both improve the model's accuracy even further. However, this implementation still has a ways to go.\n",
    "\n",
    "## Limitations and future prospects\n",
    "\n",
    "As mentioned, one of the biggest limiting factors for most of the decisions I made for this project was the ever-looming token limit. I was constantly butting up against it during testing, can could only run 1-2 tests every few minutes even with all of the token-optimizing in place. And since I do not want to compromise model quality, I believe the best solution would be to upgrade to a paid model (I have been pointed to gpt-4 as an affordable model for this scale). This would allow me to try many more things, such as the relative ordering mentioned back near the start of this notebook.\\\n",
    "\\\n",
    "As far as future prospects go, I have a lot of plans. I am currently running a small TKinter app in python to have a GUI for this application, but it is still small, ugly, and small scale, so an upgrade is definitely necessary. Additionally, getting OAuth2 to work so I can retrieve emails straight from Gmail would be ideal, and would make it a more proper application that I could more easily put on my portfolio or resume. Furthermore, in order to do all of this, I would need to purchase a database and web service for OAuth2, which could also be used to better manage user preferences, and make the API calls to the model instead of doing so from the client.\\\n",
    "\\\n",
    "So there are a lot of thing still to do for this project, and it still feels to be in it's infancy, but the road forward appears clear, and the future looks bright."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
